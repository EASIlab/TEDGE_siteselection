{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96481c62",
   "metadata": {},
   "source": [
    "# Merging Alba and Thonfeld Rasters and Extracting Western Edge Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1d81069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Workflow started ---\n",
      "STEP 1.1: Reading template bounds and reprojecting rasters...\n",
      "STEP 1.2: Combining disturbance years (Alba + Thonfeld)...\n",
      "STEP 1.3: Creating final land cover map (With Forest/Non-Forest/Disturbance)...\n",
      "  > Saving UNFILTERED land use map to: ..\\data\\edit\\Alba_dist_combined_v3\\final_landcover_map_unfiltered_10m.tif\n",
      "STEP 2.1: Filtering disturbance areas smaller than 3 pixels...\n",
      "  > 54754 disturbance areas found (before filtering).\n",
      "  > 30127 small areas removed and reclassified as Forest.\n",
      "  > Patch-ID array saved to: ..\\data\\edit\\Alba_dist_combined_v3\\final_patch_ids.npy\n",
      "  > Geotransformation saved to: ..\\data\\edit\\Alba_dist_combined_v3\\final_transform.pkl\n",
      "STEP 2.2: Finding western edge pixels of the disturbance areas...\n",
      "STEP 2.3: Filtering start pixels (no non-forest within 100m radius)...\n",
      "  > 73686 valid start pixels found after 100m filtering.\n",
      "STEP 2.4 (Pre-Cache): Creating 200m (20px) transect buffer kernel...\n",
      "  > Buffer kernel created (Size: (41, 51), Distance: 20px)\n",
      "STEP 2.4: Generating and filtering transects (kernel-based)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Transects (Raster): 100%|██████████| 73686/73686 [00:03<00:00, 22677.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > 2331 valid transects generated after raster filtering.\n",
      "  > Final transects saved to: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg\n",
      "  > Start points saved to: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg, Layer: punkte_westpixel\n",
      "--- Workflow completed ---\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import rasterio.warp\n",
    "import rasterio.features\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import scipy.ndimage as ndi\n",
    "from rasterio.enums import Resampling\n",
    "from skimage.morphology import disk\n",
    "from shapely.geometry import Point, LineString, shape\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "\n",
    "# --- Input Files ---\n",
    "# 30m ALBA-Raster (Values 1-15 = Years 2010-2024)\n",
    "ALBA_DISTURBANCE_PATH = r\"..\\data\\raw\\Alba\\disturbance_alba_epsg25832_v2.tif\"\n",
    "ALBA_DISTURBANCE_PATH = r\"..\\data\\raw\\Alba\\disturbance_alba_epsg25832_v2.tif\"\n",
    "# 10m Thonfeld-Raster (Values 1-96 = Months since Sep 2017)\n",
    "THONFELD_DISTURBANCE_PATH = r\"..\\data\\raw\\FrankThonfeld\\FCCL_092017-082025_KreiseFichtelgebirge.tif\"\n",
    "# GeoPackage defining the final extent\n",
    "TEMPLATE_GPKG_PATH = r\"..\\data\\raw\\Lancover_bkg_kreis_selected.gpkg\"\n",
    "\n",
    "# Path to the Forest/Non-Forest map (1 = Forest, 0 = Non-Forest)\n",
    "WALD_OFFENLAND_PATH = r\"..\\data\\raw\\Holzbodenkarte\\holzbodenkarte_2018_32632.tif\"\n",
    "\n",
    "# --- Output Paths ---\n",
    "OUTPUT_DIR = r\"..\\data\\edit\\Alba_dist_combined_v3\"\n",
    "\n",
    "# 1. Intermediate Product: Combined disturbance (Pixel = Year)\n",
    "OUTPUT_DISTURBANCE_YEAR_PATH = os.path.join(OUTPUT_DIR, \"disturbance_combined_pixel_year_10m.tif\")\n",
    "# 2. Intermediate Product: Final land cover map (0=Non-Forest, 1=Forest, 2=Disturbance)\n",
    "OUTPUT_LANDCOVER_PATH = os.path.join(OUTPUT_DIR, \"final_landcover_map_10m.tif\")\n",
    "# 3. Intermediate Product: Final land cover map (AFTER 3-pixel filter)\n",
    "OUTPUT_LANDCOVER_PATH = os.path.join(OUTPUT_DIR, \"final_landcover_map_filtered_10m.tif\")\n",
    "OUTPUT_LANDCOVER_UNFILTERED_PATH = os.path.join(OUTPUT_DIR, \"final_landcover_map_unfiltered_10m.tif\")\n",
    "\n",
    "# 4. Final Product: Vector lines of the transects\n",
    "OUTPUT_TRANSECTS_PATH = os.path.join(OUTPUT_DIR, \"fichtelforst.gpkg\")\n",
    "# 5. Final Product: Vector start points of the transects\n",
    "OUTPUT_START_POINTS_PATH = os.path.join(OUTPUT_DIR, \"fichtelforst.gpkg\")\n",
    "OUTPUT_START_POINTS_LAYER = \"punkte_westpixel\"\n",
    "\n",
    "\n",
    "# --- Logic Parameters ---\n",
    "TARGET_CRS = \"EPSG:25832\"\n",
    "TARGET_RES_XY = (10.0, 10.0) # 10m resolution\n",
    "NODATA_VALUE = -9999\n",
    "MIN_DISTURBANCE_SIZE_PX = 3 # Areas smaller than 3 pixels will be removed\n",
    "\n",
    "# Encoding for the final land cover map\n",
    "LC_NONFOREST = 0\n",
    "LC_FOREST = 1\n",
    "LC_DISTURBANCE = 2\n",
    "\n",
    "# Years for filtering\n",
    "ALBA_START_YEAR = 2015\n",
    "ALBA_END_YEAR = 2016\n",
    "THONFELD_START_YEAR = 2017\n",
    "\n",
    "# Distances for filtering (in pixels, at 10m resolution)\n",
    "START_POINT_BUFFER_PX = 10 # 100m\n",
    "TRANSECT_BUFFER_PX = 20 # 200m\n",
    "TRANSECT_LENGTH_PX = 10 # 100m\n",
    "\n",
    "# --- 2. Helper Functions for Year Conversion ---\n",
    "\n",
    "def map_alba_to_year(value):\n",
    "    \"\"\"Converts Alba value (1=2010) to year.\"\"\"\n",
    "    if value >= 1 and value <= 15: # 1-15\n",
    "        return float(value + 2009)\n",
    "    return 0.0\n",
    "\n",
    "def map_thonfeld_to_year(month_value):\n",
    "    \"\"\"Converts Thonfeld month value (1-96) to year.\"\"\"\n",
    "    if month_value >= 1 and month_value <= 96:\n",
    "        year = 2017 + math.floor((month_value - 1 + 8) / 12)\n",
    "        return float(year)\n",
    "    return 0.0\n",
    "\n",
    "# Vectorize the functions for Numpy arrays\n",
    "v_map_alba = np.vectorize(map_alba_to_year)\n",
    "v_map_thonfeld = np.vectorize(map_thonfeld_to_year)\n",
    "\n",
    "# --- 3. Helper Function for Raster Preparation ---\n",
    "\n",
    "def reproject_and_match(src_path, bounds, target_crs, target_res, resampling_method, nodata_val, master_profile=None):\n",
    "    \"\"\"\n",
    "    Reprojects a source raster to a target grid.\n",
    "    If master_profile=None, the grid is calculated from src (for the first call).\n",
    "    If master_profile is provided, this grid is enforced.\n",
    "    \"\"\"\n",
    "    \n",
    "    with rasterio.open(src_path) as src:\n",
    "        src_crs = src.crs\n",
    "        src_transform = src.transform\n",
    "        \n",
    "        if master_profile:\n",
    "            # Template provided\n",
    "            # Enforce the dimensions and transformation of the master.\n",
    "            dst_transform = master_profile['transform']\n",
    "            dst_width = master_profile['width']\n",
    "            dst_height = master_profile['height']\n",
    "            \n",
    "            # Create profile copy for this call\n",
    "            dst_profile = master_profile.copy()\n",
    "            dst_profile.update({\n",
    "                'dtype': np.float32,\n",
    "                'nodata': nodata_val\n",
    "            })\n",
    "        \n",
    "        else:\n",
    "            # 1st call\n",
    "            # Calculate the grid based on the bounds.\n",
    "            dst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(\n",
    "                src_crs,\n",
    "                target_crs,\n",
    "                src.width,\n",
    "                src.height,\n",
    "                *src.bounds,\n",
    "                dst_bounds=bounds,\n",
    "                resolution=target_res\n",
    "            )\n",
    "            \n",
    "            # Create the profile for the template call\n",
    "            dst_profile = {\n",
    "                'driver': 'GTiff',\n",
    "                'crs': target_crs,\n",
    "                'transform': dst_transform,\n",
    "                'width': dst_width,\n",
    "                'height': dst_height,\n",
    "                'count': 1,\n",
    "                'dtype': np.float32,\n",
    "                'nodata': nodata_val\n",
    "            }\n",
    "\n",
    "        # Initialize target array\n",
    "        dst_array = np.empty((dst_height, dst_width), dtype=np.float32)\n",
    "\n",
    "        # Perform re-projection\n",
    "        rasterio.warp.reproject(\n",
    "            source=rasterio.band(src, 1),\n",
    "            destination=dst_array,\n",
    "            src_transform=src_transform,\n",
    "            src_crs=src_crs,\n",
    "            dst_transform=dst_transform,\n",
    "            dst_crs=target_crs,\n",
    "            resampling=resampling_method,\n",
    "            dst_nodata=nodata_val\n",
    "        )\n",
    "    \n",
    "    # Return the array and the calculated or copied profile\n",
    "    return dst_array, dst_profile\n",
    "\n",
    "# --- 5. Helper Function for Buffer Kernel ---\n",
    "\n",
    "def create_transect_buffer_kernel(buffer_px, line_px):\n",
    "    \"\"\"\n",
    "    Creates a boolean mask (kernel) in the form of a \n",
    "    rounded-corner buffer around a horizontal line segment (line_px) \n",
    "    and its starting pixel (+1).\n",
    "\n",
    "    Args:\n",
    "        buffer_px (int): Buffer radius in pixels (e.g., 20 for 200m)\n",
    "        line_px (int): Length of the transect line in pixels (e.g., 10 for 100m)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Boolean 2D kernel.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The segment to be buffered (line + start pixel) is line_px + 1 pixels long\n",
    "    # e.g. (c-10) to (c) = 11 pixels\n",
    "    segment_len = line_px + 1 \n",
    "    \n",
    "    # Kernel dimensions (Bounding Box of the buffer)\n",
    "    # y-axis: buffer | center | buffer -> buffer*2 + 1\n",
    "    y_dim = buffer_px * 2 + 1\n",
    "    # x-axis: buffer | segment | buffer -> buffer*2 + segment_len\n",
    "    x_dim = buffer_px * 2 + segment_len\n",
    "    \n",
    "    # Coordinate center in the kernel\n",
    "    y_center = buffer_px\n",
    "    x_segment_start = buffer_px\n",
    "    x_segment_end = buffer_px + line_px # (Segment end is inclusive)\n",
    "    \n",
    "    # Create grid with kernel coordinates\n",
    "    J, I = np.meshgrid(np.arange(x_dim), np.arange(y_dim))\n",
    "    \n",
    "    # Calculate distance to the y_center line (axis)\n",
    "    dy = np.abs(I - y_center)\n",
    "    \n",
    "    # Calculate distance to the x-line segment (axis)\n",
    "    dx = np.zeros_like(J, dtype=float)\n",
    "    # Left of the segment\n",
    "    dx[J < x_segment_start] = x_segment_start - J[J < x_segment_start]\n",
    "    # Right of the segment\n",
    "    dx[J > x_segment_end] = J[J > x_segment_end] - x_segment_end\n",
    "    # (Above/Below the segment, dx = 0)\n",
    "    \n",
    "    # Calculate Euclidean distance to every point of the segment\n",
    "    distance = np.sqrt(dx**2 + dy**2)\n",
    "    \n",
    "    # Mask: True if distance <= buffer\n",
    "    kernel = distance <= float(buffer_px)\n",
    "    \n",
    "    print(f\"  > Buffer kernel created (Size: {kernel.shape}, Distance: {buffer_px}px)\")\n",
    "    return kernel\n",
    "\n",
    "\n",
    "# --- 4. Main Script ---\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Check if the forest map exists\n",
    "    if not os.path.exists(WALD_OFFENLAND_PATH):\n",
    "        print(f\"ATTENTION ERROR: The Forest/Non-Forest map was not found at: {WALD_OFFENLAND_PATH}\")\n",
    "        print(\"Please add the path in the configuration and restart the script.\")\n",
    "        return\n",
    "\n",
    "    print(\"--- Workflow started ---\")\n",
    "    # --- STEP 1.1: Preparation and Reprojection ---\n",
    "    print(\"STEP 1.1: Reading template bounds and reprojecting rasters...\")\n",
    "    \n",
    "    template_gdf = gpd.read_file(TEMPLATE_GPKG_PATH)\n",
    "    template_gdf = template_gdf.to_crs(TARGET_CRS)\n",
    "    target_bounds = template_gdf.total_bounds\n",
    "\n",
    "    # 1st call (ALBA): Creates the master grid. \n",
    "    # 'profile' now stores the master geometry.\n",
    "    alba_arr, profile = reproject_and_match(\n",
    "        ALBA_DISTURBANCE_PATH, target_bounds, TARGET_CRS, TARGET_RES_XY, \n",
    "        Resampling.nearest, 0 # 0 = NoData/No disturbance\n",
    "        # master_profile=None is the default here\n",
    "    )\n",
    "    \n",
    "    # 2nd call (Thonfeld): Uses the master grid from Alba\n",
    "    thonfeld_arr, _ = reproject_and_match(\n",
    "        THONFELD_DISTURBANCE_PATH, target_bounds, TARGET_CRS, TARGET_RES_XY, \n",
    "        Resampling.nearest, 0,\n",
    "        master_profile=profile\n",
    "    )\n",
    "    # replace all np.nan with 0.0\n",
    "    np.nan_to_num(thonfeld_arr, copy=False, nan=0.0)\n",
    "\n",
    "    # 3rd call (Forest): Also uses the master grid from Alba\n",
    "    wald_arr, _ = reproject_and_match(\n",
    "        WALD_OFFENLAND_PATH, target_bounds, TARGET_CRS, TARGET_RES_XY, \n",
    "        Resampling.nearest, NODATA_VALUE,\n",
    "        master_profile=profile\n",
    "    )\n",
    "    \n",
    "    # 'profile' is the master profile for all subsequent steps\n",
    "    final_transform = profile['transform']\n",
    "    final_shape = (profile['height'], profile['width'])\n",
    "    \n",
    "    # in case the 1st call (Alba) \n",
    "    # had a different dtype (although the function forces float32)\n",
    "    profile['dtype'] = np.float32\n",
    "\n",
    "    # --- STEP 1.2: Combining Disturbance Years ---\n",
    "    print(\"STEP 1.2: Combining disturbance years (Alba + Thonfeld)...\")\n",
    "    \n",
    "    alba_year_map = v_map_alba(alba_arr)\n",
    "    thonfeld_year_map = v_map_thonfeld(thonfeld_arr)\n",
    "\n",
    "    disturbance_year = np.zeros(final_shape, dtype=np.float32)\n",
    "\n",
    "    mask_thonfeld = (thonfeld_year_map >= THONFELD_START_YEAR)\n",
    "    disturbance_year[mask_thonfeld] = thonfeld_year_map[mask_thonfeld]\n",
    "\n",
    "    mask_alba = (alba_year_map >= ALBA_START_YEAR) & (alba_year_map <= ALBA_END_YEAR)\n",
    "    disturbance_year[mask_alba] = alba_year_map[mask_alba]\n",
    "\n",
    "    profile['nodata'] = 0.0\n",
    "    with rasterio.open(OUTPUT_DISTURBANCE_YEAR_PATH, 'w', **profile) as dst:\n",
    "        dst.write(disturbance_year.astype(np.float32), 1)\n",
    "\n",
    "    # --- STEP 1.3: Creating Final Land Cover Map ---\n",
    "    print(\"STEP 1.3: Creating final land cover map (With Forest/Non-Forest/Disturbance)...\")\n",
    "    \n",
    "    lc_map = np.full(final_shape, NODATA_VALUE, dtype=np.int16)\n",
    "    lc_map[wald_arr != 1] = LC_NONFOREST\n",
    "    lc_map[wald_arr == 1] = LC_FOREST\n",
    "    lc_map[disturbance_year > 0] = LC_DISTURBANCE\n",
    "\n",
    "    # Save the UNFILTERED land use map\n",
    "    print(f\"  > Saving UNFILTERED land use map to: {OUTPUT_LANDCOVER_UNFILTERED_PATH}\")\n",
    "    profile['dtype'] = np.int16\n",
    "    profile['nodata'] = NODATA_VALUE\n",
    "    with rasterio.open(OUTPUT_LANDCOVER_UNFILTERED_PATH, 'w', **profile) as dst:\n",
    "        dst.write(lc_map, 1)\n",
    "\n",
    "    # --- STEP 2.1: Filtering disturbance areas. Smaller than 3 Pix ---\n",
    "    print(f\"STEP 2.1: Filtering disturbance areas smaller than {MIN_DISTURBANCE_SIZE_PX} pixels...\")\n",
    "    \n",
    "    is_disturbance_mask = (lc_map == LC_DISTURBANCE)\n",
    "    \n",
    "    structure = ndi.generate_binary_structure(2, 2)\n",
    "    labeled_array, num_features = ndi.label(is_disturbance_mask, structure=structure)\n",
    "    \n",
    "    print(f\"  > {num_features} disturbance areas found (before filtering).\")\n",
    "\n",
    "    labels, counts = np.unique(labeled_array, return_counts=True)\n",
    "    \n",
    "    small_labels = labels[counts < MIN_DISTURBANCE_SIZE_PX]\n",
    "    small_labels = small_labels[small_labels != 0]\n",
    "\n",
    "    if len(small_labels) > 0:\n",
    "        lc_map[np.isin(labeled_array, small_labels)] = LC_FOREST\n",
    "        print(f\"  > {len(small_labels)} small areas removed and reclassified as Forest.\")\n",
    "\n",
    "    is_disturbance_cleaned = (lc_map == LC_DISTURBANCE)\n",
    "    # Recalculate label array for Step 2.4\n",
    "    labeled_array_cleaned, num_features_cleaned = ndi.label(is_disturbance_cleaned, structure=structure)\n",
    "\n",
    "    profile['dtype'] = np.int16\n",
    "    profile['nodata'] = NODATA_VALUE\n",
    "    with rasterio.open(OUTPUT_LANDCOVER_PATH, 'w', **profile) as dst:\n",
    "        dst.write(lc_map, 1)\n",
    "\n",
    "\n",
    "    # 1. Save the array with patch IDs as a NumPy file\n",
    "    patch_id_array_path = os.path.join(OUTPUT_DIR, \"final_patch_ids.npy\")\n",
    "    np.save(patch_id_array_path, labeled_array_cleaned)\n",
    "    print(f\"  > Patch-ID array saved to: {patch_id_array_path}\")\n",
    "\n",
    "    # 2. Save the geotransformation (important for x/y -> r/c)\n",
    "    # (Make the 'transform' object serializable)\n",
    "    transform_path = os.path.join(OUTPUT_DIR, \"final_transform.pkl\")\n",
    "    # 'final_transform' is a variable from Step 1.1\n",
    "    with open(transform_path, 'wb') as f:\n",
    "        pickle.dump(final_transform, f)\n",
    "    print(f\"  > Geotransformation saved to: {transform_path}\")\n",
    "\n",
    "    # --- STEP 2.2: Finding Western Edge Pixels ---\n",
    "    print(\"STEP 2.2: Finding western edge pixels of the disturbance areas...\")\n",
    "    \n",
    "    is_disturbance = (lc_map == LC_DISTURBANCE)\n",
    "    is_not_disturbance = (lc_map != LC_DISTURBANCE)\n",
    "    \n",
    "    west_neighbor_not_disturbance = np.roll(is_not_disturbance, shift=1, axis=1)\n",
    "    west_neighbor_not_disturbance[:, 0] = False\n",
    "\n",
    "    candidate_edge_pixels = is_disturbance & west_neighbor_not_disturbance\n",
    "\n",
    "    # --- STEP 2.3: Filtering Start Pixels (100m Buffer) ---\n",
    "    print(f\"STEP 2.3: Filtering start pixels (no non-forest within 100m radius)...\")\n",
    "    \n",
    "    is_non_forest = (lc_map == LC_NONFOREST)\n",
    "    footprint_100m = disk(START_POINT_BUFFER_PX)\n",
    "    dilated_non_forest_100m = ndi.binary_dilation(is_non_forest, structure=footprint_100m)\n",
    "\n",
    "    valid_start_pixels_mask = candidate_edge_pixels & ~dilated_non_forest_100m\n",
    "    \n",
    "    start_rows, start_cols = np.where(valid_start_pixels_mask)\n",
    "    print(f\"  > {len(start_rows)} valid start pixels found after 100m filtering.\")\n",
    "\n",
    "    if len(start_rows) == 0:\n",
    "        print(\"No valid start pixels found. Terminating workflow.\")\n",
    "        return\n",
    "    \n",
    "    # --- PREPARATION FOR STEP 2.4: Creating Buffer Kernel ---\n",
    "    \n",
    "    # Get array dimensions for edge checks\n",
    "    max_row, max_col = lc_map.shape\n",
    "    \n",
    "    # Define radii/lengths in pixels\n",
    "    BUFFER_RADIUS_PX = 20 # 200m\n",
    "    TRANSECT_LEN_PX_LINE = 10 # 100m\n",
    "    \n",
    "    print(\"STEP 2.4 (Pre-Cache): Creating 200m (20px) transect buffer kernel...\")\n",
    "    \n",
    "    # Create the Euclidean/rounded kernel.\n",
    "    # It buffers the 10px line AND the 1px start pixel (i.e., 11 pixel segment)\n",
    "    BUFFER_KERNEL = create_transect_buffer_kernel(BUFFER_RADIUS_PX, TRANSECT_LEN_PX_LINE)\n",
    "    \n",
    "    # Kernel offsets (center point relative to the (0,0) corner of the kernel)\n",
    "    # (r, c) (start pixel) corresponds to kernel coordinate (y_center, x_segment_end)\n",
    "    KERNEL_Y_OFFSET = BUFFER_RADIUS_PX # 20\n",
    "    # Kernel(0,0) is at (c - 10(line) - 20(buffer))\n",
    "    KERNEL_X_OFFSET_LEFT = TRANSECT_LEN_PX_LINE + BUFFER_RADIUS_PX # 10 + 20 = 30\n",
    "    \n",
    "    # --- STEP 2.4: Raster-based Transect Filtering ---\n",
    "    print(f\"STEP 2.4: Generating and filtering transects (kernel-based)...\")\n",
    "\n",
    "    final_transects_data = []\n",
    "\n",
    "    # Use tqdm for progress indication\n",
    "    for r, c in tqdm(zip(start_rows, start_cols), total=len(start_rows), desc=\"Filtering Transects (Raster)\"):\n",
    "        \n",
    "        # 1. Get ID of the current disturbance area\n",
    "        current_disturbance_id = labeled_array_cleaned[r, c]\n",
    "        \n",
    "        # 2. CHECK A (Transect Line): \n",
    "        if (c - TRANSECT_LEN_PX_LINE) < 0:\n",
    "            continue # if transect exits the raster to the left\n",
    "            \n",
    "        transect_labels = labeled_array_cleaned[r, (c - TRANSECT_LEN_PX_LINE) : c]\n",
    "        if np.any(transect_labels > 0):\n",
    "            continue\n",
    "\n",
    "        # 3. CHECK B (200m Rounded-Buffer-Check with Kernel):\n",
    "        \n",
    "        # Define the absolute bounding box of the buffer kernel\n",
    "        # Y-axis\n",
    "        r_min_abs = r - KERNEL_Y_OFFSET # r - 20\n",
    "        r_max_abs = r + KERNEL_Y_OFFSET + 1 # r + 21 (for 41 pixels)\n",
    "        \n",
    "        # X-axis\n",
    "        # Kernel(0,0) -> c - 30\n",
    "        c_min_abs = c - KERNEL_X_OFFSET_LEFT # c - 30\n",
    "        c_max_abs = c - KERNEL_X_OFFSET_LEFT + BUFFER_KERNEL.shape[1] # (c - 30) + 51 -> c + 21\n",
    "        \n",
    "        # Find valid (overlapping) indices (clipping at the raster edge)\n",
    "        r_min_valid = max(0, r_min_abs)\n",
    "        r_max_valid = min(max_row, r_max_abs)\n",
    "        c_min_valid = max(0, c_min_abs)\n",
    "        c_max_valid = min(max_col, c_max_abs)\n",
    "        \n",
    "        # If the slice has invalid dimensions (e.g., 0), skip\n",
    "        if (r_max_valid <= r_min_valid) or (c_max_valid <= c_min_valid):\n",
    "            continue\n",
    "\n",
    "        # Calculate the corresponding slices for the kernel itself\n",
    "        k_r_min = r_min_valid - r_min_abs\n",
    "        k_r_max = k_r_min + (r_max_valid - r_min_valid)\n",
    "        k_c_min = c_min_valid - c_min_abs\n",
    "        k_c_max = k_c_min + (c_max_valid - c_min_valid)\n",
    "\n",
    "        # Get the slices from the main arrays and the kernel\n",
    "        try:\n",
    "            lc_map_window = lc_map[r_min_valid:r_max_valid, c_min_valid:c_max_valid]\n",
    "            labels_window = labeled_array_cleaned[r_min_valid:r_max_valid, c_min_valid:c_max_valid]\n",
    "            kernel_window = BUFFER_KERNEL[k_r_min:k_r_max, k_c_min:k_c_max]\n",
    "        except IndexError:\n",
    "            # Safety catch in case slicing fails\n",
    "            continue\n",
    "\n",
    "        # CHECK B.1: Is Non-Forest (LC_NONFOREST) in the kernel buffer?\n",
    "        # Apply boolean mask (kernel_window) to the array\n",
    "        if np.any(lc_map_window[kernel_window] == LC_NONFOREST):\n",
    "            continue\n",
    "\n",
    "        # CHECK B.2: Is ANOTHER disturbance in the kernel buffer?\n",
    "        # Apply mask to get only pixels within the buffer\n",
    "        labels_in_buffer = labels_window[kernel_window]\n",
    "        \n",
    "        # Check these pixels for disturbance areas other than the current one\n",
    "        other_disturbance_mask = (labels_in_buffer > 0) & (labels_in_buffer != current_disturbance_id)\n",
    "        if np.any(other_disturbance_mask):\n",
    "            continue\n",
    "\n",
    "        # --- If all checks are passed ---\n",
    "        # Create vector data (points, lines)\n",
    "        start_x, start_y = final_transform * (c + 0.5, r + 0.5)\n",
    "        # 100m = TRANSECT_LEN_PX_LINE (10)\n",
    "        end_x, end_y = final_transform * (c - TRANSECT_LEN_PX_LINE + 0.5, r + 0.5)\n",
    "        \n",
    "        start_point_geom = Point(start_x, start_y)\n",
    "        transect_line_geom = LineString([start_point_geom, Point(end_x, end_y)])\n",
    "\n",
    "        final_transects_data.append({\n",
    "            'geometry': transect_line_geom,\n",
    "            'start_point_geom': start_point_geom,\n",
    "            'disturbance_id': current_disturbance_id,\n",
    "            'start_row': r,\n",
    "            'start_col': c\n",
    "        })\n",
    "\n",
    "    print(f\"  > {len(final_transects_data)} valid transects generated after raster filtering.\")\n",
    "\n",
    "    # --- STEP 2.5: Saving Results ---\n",
    "    if final_transects_data:\n",
    "        # Save transects\n",
    "        transects_gdf = gpd.GeoDataFrame(\n",
    "            final_transects_data, \n",
    "            geometry='geometry', \n",
    "            crs=TARGET_CRS\n",
    "        )\n",
    "        transects_gdf.drop(columns=['start_point_geom'], inplace=True)\n",
    "        transects_gdf.to_file(OUTPUT_TRANSECTS_PATH, driver=\"GPKG\")\n",
    "        print(f\"  > Final transects saved to: {OUTPUT_TRANSECTS_PATH}\")\n",
    "\n",
    "        # Save start points\n",
    "        start_points_gdf = gpd.GeoDataFrame(\n",
    "            final_transects_data, \n",
    "            geometry='start_point_geom', \n",
    "            crs=TARGET_CRS\n",
    "        )\n",
    "        start_points_gdf.drop(columns=['geometry'], inplace=True)\n",
    "        start_points_gdf.to_file(OUTPUT_START_POINTS_PATH, layer=OUTPUT_START_POINTS_LAYER, driver=\"GPKG\")\n",
    "        print(f\"  > Start points saved to: {OUTPUT_START_POINTS_PATH}, Layer: {OUTPUT_START_POINTS_LAYER}\")\n",
    "    else:\n",
    "        print(\"No transects met all filter criteria.\")\n",
    "\n",
    "    print(\"--- Workflow completed ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0eaaf9",
   "metadata": {},
   "source": [
    "# Determine Disturbance Area in Hectares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6040495c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting area calculation for disturbance points ---\n",
      "Reading raster: ..\\data\\edit\\Alba_dist_combined_v3\\disturbance_combined_pixel_year_10m.tif\n",
      "  Raster CRS: EPSG:25832, Pixel Area: 100.0 m² (0.01 ha)\n",
      "Segmenting disturbance areas...\n",
      "  54754 connected disturbance areas found.\n",
      "Calculating area per segment...\n",
      "  Areas calculated for 54754 segments.\n",
      "  Min. Area: 0.0100 ha, Max. Area: 284.90 ha\n",
      "Reading point file: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg\n",
      "  Using layer: punkte_westpixel\n",
      "  2331 points found.\n",
      "  Points CRS: EPSG:25832\n",
      "Extracting area IDs for each point...\n",
      "  Patch IDs extracted for 2331 points.\n",
      "Assigning areas to the new column 'Area_ha'...\n",
      "Saving result to: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg layer: punkte_westpixel_ha...\n",
      "  Successfully saved.\n",
      "\n",
      "---------------------------------\n",
      "Script finished successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.transform\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "# Path to the disturbance raster (Pixels 2015-2025 are disturbances)\n",
    "RASTER_PATH = r\"..\\data\\edit\\Alba_dist_combined_v3\\disturbance_combined_pixel_year_10m.tif\"\n",
    "\n",
    "# --- Path to Point File\n",
    "POINTS_PATH = r\"..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg\"\n",
    "POINTS_LAYER_NAME = \"punkte_westpixel\" \n",
    "\n",
    "# Name of the new column for the area in hectares\n",
    "AREA_COLUMN_NAME = 'Area_ha'\n",
    "\n",
    "# Name for the output file (will be saved in the same folder as the point file)\n",
    "OUTPUT_LAYER_NAME = \"punkte_westpixel_ha\"\n",
    "\n",
    "\n",
    "# --- 2. Main Script ---\n",
    "\n",
    "def add_patch_area_to_points():\n",
    "    print(\"--- Starting area calculation for disturbance points ---\")\n",
    "    \n",
    "    # --- Step 1: Load raster and segment disturbance areas ---\n",
    "    try:\n",
    "        print(f\"Reading raster: {RASTER_PATH}\")\n",
    "        with rasterio.open(RASTER_PATH) as src:\n",
    "            data = src.read(1)\n",
    "            transform = src.transform\n",
    "            crs = src.crs\n",
    "            profile = src.profile # Metadata for later use\n",
    "            \n",
    "            # Calculate pixel area\n",
    "            pixel_area_m2 = abs(transform.a * transform.e)\n",
    "            if pixel_area_m2 == 0:\n",
    "                print(\"Attention !!! ERROR: Pixel area is 0. Transformation invalid.\")\n",
    "                return False\n",
    "            pixel_area_ha = pixel_area_m2 / 10000.0\n",
    "            print(f\"  Raster CRS: {crs}, Pixel Area: {pixel_area_m2} m² ({pixel_area_ha} ha)\")\n",
    "\n",
    "            # Mask for disturbance pixels (1-2026)\n",
    "            clearing_mask = (data >= 1) & (data <= 2026)\n",
    "            \n",
    "            # Segmenting disturbance areas\n",
    "            print(\"Segmenting disturbance areas...\")\n",
    "            # `structure` defines neighborhood (here 8-connectivity)\n",
    "            structure = ndi.generate_binary_structure(2, 2) \n",
    "            labeled_patches, num_features = ndi.label(clearing_mask, structure=structure)\n",
    "            print(f\"  {num_features} connected disturbance areas found.\")\n",
    "\n",
    "            if num_features == 0:\n",
    "                print(\"ATTENTION ERROR!?: No disturbance areas found in the raster.\")\n",
    "            \n",
    "            # IMPORTANT: Keep transform and labeled_patches in memory for later use\n",
    "            # (they will be needed after the context manager closes)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Raster file not found: {RASTER_PATH}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR while reading or segmenting the raster: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- Step 2: Calculate area per segment ---\n",
    "    print(\"Calculating area per segment...\")\n",
    "    patch_areas_ha = {} # Dictionary: {patch_id: area_in_ha}\n",
    "    \n",
    "    if num_features > 0:\n",
    "        # Find all unique patch IDs (except 0 = background)\n",
    "        unique_labels = np.unique(labeled_patches)\n",
    "        valid_labels = unique_labels[unique_labels != 0]\n",
    "        \n",
    "        # Count pixels per patch ID\n",
    "        labels, counts = np.unique(labeled_patches, return_counts=True)\n",
    "        \n",
    "        # Create the dictionary\n",
    "        for label, count in zip(labels, counts):\n",
    "            if label != 0: # Ignore forest\n",
    "                 patch_areas_ha[label] = round(count * pixel_area_ha, 4)\n",
    "                 \n",
    "        print(f\"  Areas calculated for {len(patch_areas_ha)} segments.\")\n",
    "        \n",
    "        # Optional: Output Min/Max area\n",
    "        if patch_areas_ha:\n",
    "             min_area = min(patch_areas_ha.values())\n",
    "             max_area = max(patch_areas_ha.values())\n",
    "             print(f\"  Min. Area: {min_area:.4f} ha, Max. Area: {max_area:.2f} ha\")\n",
    "        \n",
    "    # --- Step 3: Load points and check/adjust CRS ---\n",
    "    try:\n",
    "        print(f\"Reading point file: {POINTS_PATH}\")\n",
    "        points_layer_name = None\n",
    "        if 'POINTS_LAYER_NAME' in globals():\n",
    "             points_layer_name = POINTS_LAYER_NAME\n",
    "             print(f\"  Using layer: {points_layer_name}\")\n",
    "             \n",
    "        points_gdf = gpd.read_file(POINTS_PATH, layer=points_layer_name)\n",
    "        print(f\"  {len(points_gdf)} points found.\")\n",
    "        print(f\"  Points CRS: {points_gdf.crs}\")\n",
    "        \n",
    "        # Compare CRS and adjust if necessary\n",
    "        if points_gdf.crs != crs:\n",
    "            print(f\"WARNING: CRS of points ({points_gdf.crs}) differs from raster CRS ({crs}). Reprojecting points...\")\n",
    "            try:\n",
    "                points_gdf = points_gdf.to_crs(crs)\n",
    "                print(f\"  Points successfully reprojected to {crs}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during CRS transformation of points: {e}\")\n",
    "                print(\"Ensure both layers have valid CRS.\")\n",
    "                return False\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Point file not found: {POINTS_PATH}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR while reading the point file: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- Step 4: Extract area for each point ---\n",
    "    print(\"Extracting area IDs for each point...\")\n",
    "    point_patch_ids = []\n",
    "    \n",
    "    # Extract coordinates as a list of tuples\n",
    "    coords = [(p.x, p.y) for p in points_gdf.geometry]\n",
    "    \n",
    "    # Use the transform stored in memory to extract patch IDs\n",
    "    try:\n",
    "        # Sample directly from the labeled_patches array using the transform\n",
    "        for x, y in coords:\n",
    "            # Convert coordinates to row/column using the transform\n",
    "            row, col = rasterio.transform.rowcol(transform, x, y)\n",
    "            # Check if index is within raster bounds\n",
    "            if 0 <= row < labeled_patches.shape[0] and 0 <= col < labeled_patches.shape[1]:\n",
    "                point_patch_ids.append(int(labeled_patches[row, col]))\n",
    "            else:\n",
    "                point_patch_ids.append(0) # Point outside -> ID 0\n",
    "        print(f\"  Patch IDs extracted for {len(point_patch_ids)} points.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "         print(f\"ERROR while sampling raster values at point locations: {e}\")\n",
    "         # Fallback\n",
    "         print(\"Attempting manual sampling (slower)...\")\n",
    "         point_patch_ids = []\n",
    "         try:\n",
    "             for x, y in coords:\n",
    "                 # Convert coordinates to row/column\n",
    "                 row, col = rasterio.transform.rowcol(transform, x, y)\n",
    "                 # Check if index is within raster bounds\n",
    "                 if 0 <= row < labeled_patches.shape[0] and 0 <= col < labeled_patches.shape[1]:\n",
    "                      point_patch_ids.append(int(labeled_patches[row, col]))\n",
    "                 else:\n",
    "                      point_patch_ids.append(0) # Point outside -> ID 0\n",
    "             print(f\"  Manual sampling for {len(point_patch_ids)} points completed.\")\n",
    "         except Exception as e_inner:\n",
    "             print(f\"ERROR also during manual sampling: {e_inner}\")\n",
    "             return False\n",
    "\n",
    "    # --- Step 5: Assign areas and save ---\n",
    "    print(f\"Assigning areas to the new column '{AREA_COLUMN_NAME}'...\")\n",
    "    \n",
    "    # Use the extracted patch IDs to get the areas from the dictionary\n",
    "    # .get(id, 0) returns 0 if the ID is not in the dict (e.g., for ID 0 or errors)\n",
    "    areas_for_points = [patch_areas_ha.get(patch_id, 0) for patch_id in point_patch_ids]\n",
    "    \n",
    "    # New column to GeoDataFrame\n",
    "    points_gdf[AREA_COLUMN_NAME] = areas_for_points\n",
    "    points_gdf.head()\n",
    "    \n",
    "    # Create path for the output file\n",
    "    output_path = POINTS_PATH\n",
    "    \n",
    "    try:\n",
    "        print(f\"Saving result to: {output_path} layer: {OUTPUT_LAYER_NAME}...\")\n",
    "        points_gdf.to_file(output_path, layer=OUTPUT_LAYER_NAME, driver=\"GPKG\")\n",
    "        print(\"  Successfully saved.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR while saving the result file: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- 3. Execute Script ---\n",
    "if __name__ == \"__main__\":\n",
    "    success = add_patch_area_to_points()\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"\\n---------------------------------\")\n",
    "    if success:\n",
    "        print(\"Script finished successfully.\")\n",
    "    else:\n",
    "        print(\"Script finished with errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc58072b",
   "metadata": {},
   "source": [
    "# Add Topographic (TWI, Slope, Height) and Disturbance (Year) Metrics\n",
    "\n",
    "Metrics are calculated at three scales:\n",
    "- Value at the exact point location\n",
    "- Median value of the 100m neighborhood (within the same patch)\n",
    "- Median value of the entire disturbance patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c45fd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Extended statistics script started ---\n",
      "Loading points: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg\n",
      "Loading Patch-ID array: ..\\data\\edit\\Alba_dist_combined_v3\\final_patch_ids.npy\n",
      "Loading Master-Transformation: ..\\data\\edit\\Alba_dist_combined_v3\\final_transform.pkl\n",
      "Master CRS (from 'Year' raster) determined: EPSG:25832\n",
      "Points are already in Master CRS.\n",
      "Loading raster array: Hoehe...\n",
      "Loading raster array: SCA...\n",
      "Loading raster array: Slope...\n",
      "Loading raster array: TWI...\n",
      "Loading raster array: Year...\n",
      "All data loaded.\n",
      "Calculating pixel coordinates (r, c) for all points...\n",
      "Pixel coordinates calculated successfully.\n",
      "--- Processing raster: Hoehe ---\n",
      "Calculating Metric 1: Point value...\n",
      "  > Reprojecting raster 'Hoehe' to Master Grid...\n",
      "Calculating Metric 2: Median of the entire patch area (Zonal)...\n",
      "Calculating Metric 3: Median of the 100m neighborhood (on patch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local Medians (Hoehe): 100%|██████████| 2331/2331 [00:00<00:00, 8834.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing raster: SCA ---\n",
      "Calculating Metric 1: Point value...\n",
      "  > Reprojecting raster 'SCA' to Master Grid...\n",
      "Calculating Metric 2: Median of the entire patch area (Zonal)...\n",
      "Calculating Metric 3: Median of the 100m neighborhood (on patch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local Medians (SCA): 100%|██████████| 2331/2331 [00:00<00:00, 7130.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing raster: Slope ---\n",
      "Calculating Metric 1: Point value...\n",
      "  > Reprojecting raster 'Slope' to Master Grid...\n",
      "Calculating Metric 2: Median of the entire patch area (Zonal)...\n",
      "Calculating Metric 3: Median of the 100m neighborhood (on patch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local Medians (Slope): 100%|██████████| 2331/2331 [00:00<00:00, 9618.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing raster: TWI ---\n",
      "Calculating Metric 1: Point value...\n",
      "  > Reprojecting raster 'TWI' to Master Grid...\n",
      "Calculating Metric 2: Median of the entire patch area (Zonal)...\n",
      "Calculating Metric 3: Median of the 100m neighborhood (on patch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local Medians (TWI): 100%|██████████| 2331/2331 [00:00<00:00, 7423.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing raster: Year ---\n",
      "Calculating Metric 1: Point value...\n",
      "  > Raster grid already matches. Skipping reprojection.\n",
      "Calculating Metric 2: Median of the entire patch area (Zonal)...\n",
      "Calculating Metric 3: Median of the 100m neighborhood (on patch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local Medians (Year): 100%|██████████| 2331/2331 [00:00<00:00, 7609.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- All calculations completed ---\n",
      "Saving final GeoPackage file to: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg, Layer: punkte_westpixel_ha_stats\n",
      "--- Script finished successfully ---\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.warp\n",
    "from rasterio.enums import Resampling\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.ndimage import median as nd_median\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(\"--- Extended statistics script started ---\")\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "# Directory Paths\n",
    "BASE_DIR = r\"..\\data\\edit\\Alba_dist_combined_v3\"\n",
    "WBT_DIR = r\"..\\data\\edit\\DGM\\WBT_Output\"\n",
    "DEM_PATH = r\"..\\data\\edit\\DGM\\DGM_merg_10m.tif\"\n",
    "\n",
    "# Input: Points with area\n",
    "POINTS_PATH = os.path.join(BASE_DIR, \"fichtelforst.gpkg\")\n",
    "POINTS_LAYER = \"punkte_westpixel_ha\"\n",
    "\n",
    "\n",
    "# Input: Files from the previous processing step\n",
    "PATCH_ID_ARRAY_PATH = os.path.join(BASE_DIR, \"final_patch_ids.npy\")\n",
    "TRANSFORM_PATH = os.path.join(BASE_DIR, \"final_transform.pkl\")\n",
    "\n",
    "# Input: Rasters to be sampled\n",
    "RASTER_TO_SAMPLE = {\n",
    "    \"Hoehe\": DEM_PATH,\n",
    "    \"SCA\": os.path.join(WBT_DIR, \"02_wbt_sca.tif\"),\n",
    "    \"Slope\": os.path.join(WBT_DIR, \"03_wbt_slope_radians.tif\"),\n",
    "    \"TWI\": os.path.join(WBT_DIR, \"04_wbt_TWI.tif\"),\n",
    "    \"Year\": os.path.join(BASE_DIR, \"disturbance_combined_pixel_year_10m.tif\")\n",
    "}\n",
    "\n",
    "# Parameters for neighborhood median\n",
    "NEIGHBORHOOD_RADIUS_PX = 10 # 10 pixels = 100m\n",
    "\n",
    "# Output\n",
    "OUTPUT_PATH = POINTS_PATH\n",
    "OUTPUT_LAYER = \"punkte_westpixel_ha_stats\"\n",
    "\n",
    "\n",
    "# --- 2. Load Data ---\n",
    "\n",
    "print(f\"Loading points: {POINTS_PATH}\")\n",
    "try:\n",
    "    points_gdf = gpd.read_file(POINTS_PATH, layer=POINTS_LAYER)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load layer '{POINTS_LAYER}'. Attempting to load 'valid_transect_start_points' instead...\")\n",
    "    try:\n",
    "        points_gdf = gpd.read_file(\n",
    "            os.path.join(BASE_DIR, \"valid_transect_start_points.gpkg\"), \n",
    "            layer=\"valid_transect_start_points\"\n",
    "        )\n",
    "    except Exception as e_inner:\n",
    "        print(f\"Could not load original points Gpkg: {e_inner}\")\n",
    "        exit()\n",
    "\n",
    "print(f\"Loading Patch-ID array: {PATCH_ID_ARRAY_PATH}\")\n",
    "patch_id_array = np.load(PATCH_ID_ARRAY_PATH)\n",
    "\n",
    "print(f\"Loading Master-Transformation: {TRANSFORM_PATH}\")\n",
    "with open(TRANSFORM_PATH, 'rb') as f:\n",
    "    # Master transformation\n",
    "    transform = pickle.load(f)\n",
    "\n",
    "# --- Determine Master CRS ---\n",
    "master_crs = None\n",
    "year_raster_path = RASTER_TO_SAMPLE.get(\"Year\")\n",
    "if not year_raster_path:\n",
    "    print(\"ERROR: 'Year' raster must be defined in RASTER_TO_SAMPLE.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    with rasterio.open(year_raster_path) as src:\n",
    "        master_crs = src.crs\n",
    "        print(f\"Master CRS (from 'Year' raster) determined: {master_crs}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not open 'Year' raster to read Master CRS: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Reproject points to Master CRS ---\n",
    "if points_gdf.crs != master_crs:\n",
    "    print(f\"Reprojecting points from {points_gdf.crs} to {master_crs}...\")\n",
    "    points_gdf = points_gdf.to_crs(master_crs)\n",
    "else:\n",
    "    print(\"Points are already in Master CRS.\")\n",
    "\n",
    "# Load all raster arrays\n",
    "raster_data = {}\n",
    "for name, path in RASTER_TO_SAMPLE.items():\n",
    "    print(f\"Loading raster array: {name}...\")\n",
    "    with rasterio.open(path) as src:\n",
    "        raster_data[name] = {\n",
    "            \"array\": src.read(1),\n",
    "            \"nodata\": src.nodata\n",
    "        }\n",
    "\n",
    "print(\"All data loaded.\")\n",
    "\n",
    "# --- 3. Calculate Pixel Coordinates (r, c) for points ---\n",
    "print(\"Calculating pixel coordinates (r, c) for all points...\")\n",
    "\n",
    "# since points_gdf is in Master CRS, they are (x, y)\n",
    "coords_xy_master = [(p.x, p.y) for p in points_gdf.geometry]\n",
    "\n",
    "# and 'transform' is the Master transformation, they are (r, c)\n",
    "px_coords_master = [rasterio.transform.rowcol(transform, x, y) for x, y in coords_xy_master]\n",
    "\n",
    "# Save the indices in the GDF\n",
    "points_gdf['px_row'] = [r for r, c in px_coords_master]\n",
    "points_gdf['px_col'] = [c for r, c in px_coords_master]\n",
    "\n",
    "print(\"Pixel coordinates calculated successfully.\")\n",
    "\n",
    "# --- 4. Calculate Metrics ---\n",
    "# Array dimensions and the profile of the Master Grid\n",
    "max_row, max_col = patch_id_array.shape\n",
    "master_profile = {\n",
    "    'driver': 'GTiff',\n",
    "    'crs': master_crs,\n",
    "    'transform': transform,\n",
    "    'width': max_col,\n",
    "    'height': max_row,\n",
    "    'count': 1,\n",
    "    'dtype': np.float32, # Target dtype for reprojection\n",
    "    'nodata': -9999\n",
    "}\n",
    "master_shape = (max_row, max_col)\n",
    "\n",
    "# Coordinates for rasterio.sample\n",
    "coords = [(p.x, p.y) for p in points_gdf.geometry]\n",
    "\n",
    "# Iterate over each raster to be sampled\n",
    "for name, data in raster_data.items():\n",
    "    print(f\"--- Processing raster: {name} ---\")\n",
    "    \n",
    "    # Get the source data\n",
    "    source_array = data['array']\n",
    "    source_nodata = data['nodata']\n",
    "    \n",
    "    # --- METRIC 1: Value at the exact pixel (via rasterio.sample) ---\n",
    "    print(\"Calculating Metric 1: Point value...\")\n",
    "    col_name_point = f\"{name}_Point\"\n",
    "    \n",
    "    # Re-open the source file to use .sample()\n",
    "    with rasterio.open(RASTER_TO_SAMPLE[name]) as src:\n",
    "        # src.sample() is robust against grid mismatches\n",
    "        sampled_values = list(src.sample(coords))\n",
    "        point_values = [val[0] for val in sampled_values]\n",
    "        \n",
    "    points_gdf[col_name_point] = point_values\n",
    "\n",
    "    # --- Preparation for Metric 2 & 3: Reprojection ---\n",
    "    \n",
    "    # Check if the source raster is already on the master grid\n",
    "    if source_array.shape == master_shape:\n",
    "        print(\"  > Raster grid already matches. Skipping reprojection.\")\n",
    "        current_array_aligned = source_array\n",
    "        current_nodata_aligned = source_nodata\n",
    "    else:\n",
    "        print(f\"  > Reprojecting raster '{name}' to Master Grid...\")\n",
    "        \n",
    "        # Create an empty array with the target shape\n",
    "        current_array_aligned = np.empty(master_shape, dtype=master_profile['dtype'])\n",
    "        current_nodata_aligned = master_profile['nodata']\n",
    "\n",
    "        # Reproject the source array\n",
    "        with rasterio.open(RASTER_TO_SAMPLE[name]) as src:\n",
    "            rasterio.warp.reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=current_array_aligned,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=master_profile['transform'],\n",
    "                dst_crs=master_profile['crs'],\n",
    "                resampling=Resampling.bilinear, # 'nearest' for categorical data\n",
    "                dst_nodata=current_nodata_aligned\n",
    "            )\n",
    "\n",
    "    # --- METRIC 2: Median of the entire disturbance area (Zonal statistics) ---\n",
    "    print(\"Calculating Metric 2: Median of the entire patch area (Zonal)...\")\n",
    "    col_name_patch = f\"{name}_Patch_Median\"\n",
    "    \n",
    "    # Mask for valid pixels (in the reprojected array)\n",
    "    valid_mask = (current_array_aligned != current_nodata_aligned) & \\\n",
    "                 (current_array_aligned != source_nodata) & \\\n",
    "                 (patch_id_array > 0)\n",
    "    \n",
    "    patch_ids_unique = np.unique(patch_id_array[patch_id_array > 0])\n",
    "    \n",
    "    if len(patch_ids_unique) > 0:\n",
    "        # Calculate medians\n",
    "        median_values = nd_median(\n",
    "            current_array_aligned[valid_mask], \n",
    "            labels=patch_id_array[valid_mask], \n",
    "            index=patch_ids_unique\n",
    "        )\n",
    "        # Create lookup table\n",
    "        patch_median_lookup = dict(zip(patch_ids_unique, median_values))\n",
    "    else:\n",
    "        patch_median_lookup = {}\n",
    "    \n",
    "    # Assign the median value based on the point's patch ID\n",
    "    # (Requires the (r,c) coordinates of the MASTER grid)\n",
    "    px_coords_master = [(r, c) for r, c in zip(points_gdf['px_row'], points_gdf['px_col'])]\n",
    "    point_patch_ids = [patch_id_array[r, c] for r, c in px_coords_master]\n",
    "    points_gdf[col_name_patch] = [patch_median_lookup.get(pid, np.nan) for pid in point_patch_ids]\n",
    "\n",
    "    # --- METRIC 3: Median of the 100m NEIGHBORHOOD (only on patch) ---\n",
    "    print(\"Calculating Metric 3: Median of the 100m neighborhood (on patch)...\")\n",
    "    col_name_local = f\"{name}_Neighborhood_Median\"\n",
    "    \n",
    "    local_median_values = []\n",
    "    \n",
    "    for r, c in tqdm(px_coords_master, desc=f\"Local Medians ({name})\"):\n",
    "        try:\n",
    "            current_patch_id = patch_id_array[r, c]\n",
    "            if current_patch_id == 0: \n",
    "                local_median_values.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            # Window (10-pixel radius)\n",
    "            r_min = max(0, r - NEIGHBORHOOD_RADIUS_PX)\n",
    "            r_max = min(max_row, r + NEIGHBORHOOD_RADIUS_PX + 1)\n",
    "            c_min = max(0, c - NEIGHBORHOOD_RADIUS_PX)\n",
    "            c_max = min(max_col, c + NEIGHBORHOOD_RADIUS_PX + 1)\n",
    "            \n",
    "            # Get slices (from the ALIGNED array)\n",
    "            patch_id_window = patch_id_array[r_min:r_max, c_min:c_max]\n",
    "            value_window = current_array_aligned[r_min:r_max, c_min:c_max]\n",
    "            \n",
    "            # Mask: \n",
    "            mask = (patch_id_window == current_patch_id) & \\\n",
    "                   (value_window != current_nodata_aligned) & \\\n",
    "                   (value_window != source_nodata)\n",
    "            \n",
    "            if np.any(mask):\n",
    "                local_median_values.append(np.median(value_window[mask]))\n",
    "            else:\n",
    "                local_median_values.append(np.nan)\n",
    "        \n",
    "        except Exception:\n",
    "            local_median_values.append(np.nan)\n",
    "            \n",
    "    points_gdf[col_name_local] = local_median_values\n",
    "\n",
    "print(\"--- All calculations completed ---\")\n",
    "\n",
    "# --- 5. Saving ---\n",
    "print(f\"Saving final GeoPackage file to: {OUTPUT_PATH}, Layer: {OUTPUT_LAYER}\")\n",
    "# Drop temporary columns\n",
    "if 'px_row' in points_gdf.columns:\n",
    "    points_gdf = points_gdf.drop(columns=['px_row', 'px_col'])\n",
    "\n",
    "points_gdf.to_file(OUTPUT_PATH, layer=OUTPUT_LAYER, driver=\"GPKG\")\n",
    "print(\"--- Script finished successfully ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c203c1fd",
   "metadata": {},
   "source": [
    "# Filter by Roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d873e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting filtering by road proximity (100m) ---\n",
      "Reading points: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg (Layer: punkte_westpixel_ha_stats)\n",
      "  2331 points loaded.\n",
      "Reading roads: ..\\data\\edit\\OeSM\\OeSM_road_25835.gpkg (Layer: OeSM_road_25835)\n",
      "  114344 road segments loaded.\n",
      "  46569 interfering road segments found.\n",
      "Creating 100m buffer around interfering roads...\n",
      "Generating 100m transects (West) from start points...\n",
      "Performing spatial filtering (Checking lines for 'intersects')...\n",
      "  FILTERING COMPLETED: 661 of 2331 points/transects retained.\n",
      "Saving filtered POINTS to: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg (Layer: noRoads_points_100m)\n",
      "Saving filtered TRANSECTS to: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg (Layer: noRoads_trans_100m)\n",
      "Saving road buffer (Exclusion Zone) to: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg layer: roadsBuffer100\n",
      "  Successfully saved.\n",
      "\n",
      "---------------------------------\n",
      "Script finished successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "# --- Paths for Input Points ---\n",
    "POINTS_PATH = r\"..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg\"\n",
    "INPUT_LAYER_NAME = \"punkte_westpixel_ha_stats\" \n",
    "\n",
    "# --- Paths Roads ---\n",
    "ROADS_PATH = r\"..\\data\\edit\\OeSM\\OeSM_road_25835.gpkg\"\n",
    "ROADS_LAYER_NAME = \"OeSM_road_25835\"\n",
    "\n",
    "# --- Filter Parameters ---\n",
    "BUFFER_DISTANCE_METERS = 100\n",
    "TRANSECT_LENGTH_METERS = 100\n",
    "ROADS_TO_EXCLUDE = [\n",
    "    'primary', 'service', 'track_grade2', 'track_grade3', 'track_grade4'\n",
    "]\n",
    "\n",
    "# --- Output Names ---\n",
    "OUTPUT_POINTS_LAYER_NAME = f\"noRoads_points_{int(BUFFER_DISTANCE_METERS)}m\"\n",
    "OUTPUT_TRANSECTS_LAYER_NAME = f\"noRoads_trans_{int(BUFFER_DISTANCE_METERS)}m\"\n",
    "OUTPUT_BUFFER_PATH = r\"..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg\"\n",
    "\n",
    "\n",
    "# --- 2. Main Script ---\n",
    "\n",
    "def filter_transects_near_roads():\n",
    "    print(f\"--- Starting filtering by road proximity ({BUFFER_DISTANCE_METERS}m) ---\")\n",
    "\n",
    "    # --- Step 1: Load Points ---\n",
    "    try:\n",
    "        print(f\"Reading points: {POINTS_PATH} (Layer: {INPUT_LAYER_NAME})\")\n",
    "        gdf_points = gpd.read_file(POINTS_PATH, layer=INPUT_LAYER_NAME)\n",
    "        print(f\"  {len(gdf_points)} points loaded.\")\n",
    "        points_crs = gdf_points.crs\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR while reading the point file/layer: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- Step 2: Load and Filter Roads ---\n",
    "    try:\n",
    "        print(f\"Reading roads: {ROADS_PATH} (Layer: {ROADS_LAYER_NAME})\")\n",
    "        gdf_roads = gpd.read_file(ROADS_PATH, layer=ROADS_LAYER_NAME)\n",
    "        print(f\"  {len(gdf_roads)} road segments loaded.\")\n",
    "        \n",
    "        if gdf_roads.crs != points_crs:\n",
    "            print(f\"  WARNING: CRS-Mismatch. Reprojecting roads to {points_crs}...\")\n",
    "            gdf_roads = gdf_roads.to_crs(points_crs)\n",
    "            \n",
    "        gdf_roads_filtered = gdf_roads[gdf_roads['fclass'].isin(ROADS_TO_EXCLUDE)]\n",
    "        \n",
    "        if gdf_roads_filtered.empty:\n",
    "            print(\"INFO: None of the road types to be excluded were found. No filtering necessary.\")\n",
    "            gdf_points.to_file(POINTS_PATH, layer=OUTPUT_POINTS_LAYER_NAME, driver=\"GPKG\")\n",
    "            # Generate lines\n",
    "            generate_and_save_transects(gdf_points, POINTS_PATH, OUTPUT_TRANSECTS_LAYER_NAME)\n",
    "            print(\"Original points and transects saved under new layers.\")\n",
    "            return True\n",
    "            \n",
    "        print(f\"  {len(gdf_roads_filtered)} interfering road segments found.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR while loading or filtering roads: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- Step 3: Create Buffer (Exclusion Zone) ---\n",
    "    try:\n",
    "        print(f\"Creating {BUFFER_DISTANCE_METERS}m buffer around interfering roads...\")\n",
    "        roads_buffer = gdf_roads_filtered.buffer(BUFFER_DISTANCE_METERS)\n",
    "        exclusion_zone = roads_buffer.union_all()\n",
    "        exclusion_gdf = gpd.GeoDataFrame(geometry=[exclusion_zone], crs=points_crs)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR while buffering roads: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- Step 4: Generate Transects from Points ---\n",
    "    print(f\"Generating {TRANSECT_LENGTH_METERS}m transects (West) from start points...\")\n",
    "    transect_geometries = []\n",
    "    # Keep the index of the points for association\n",
    "    original_index = gdf_points.index\n",
    "    \n",
    "    for point in gdf_points.geometry:\n",
    "        start_x, start_y = point.x, point.y\n",
    "        # Line to the West\n",
    "        end_x = start_x - TRANSECT_LENGTH_METERS\n",
    "        end_y = start_y\n",
    "        transect_geometries.append(LineString([(start_x, start_y), (end_x, end_y)]))\n",
    "\n",
    "    gdf_transects = gpd.GeoDataFrame(geometry=transect_geometries, crs=points_crs)\n",
    "    # Index to link the lines with the points\n",
    "    gdf_transects.index = original_index\n",
    "\n",
    "    # --- Step 5: Filter Transects ---\n",
    "    try:\n",
    "        print(\"Performing spatial filtering (Checking lines for 'intersects')...\")\n",
    "        \n",
    "        # Find all transects that INTERSECT the buffer zone\n",
    "        intersecting_transects = gpd.sjoin(gdf_transects, exclusion_gdf, predicate='intersects')\n",
    "        \n",
    "        # Identify the indices of unsuitable transects\n",
    "        bad_indices = intersecting_transects.index\n",
    "        \n",
    "        # Create a mask for valid indices (those NOT in the 'bad_indices'/unsuitable list)\n",
    "        good_mask = ~gdf_points.index.isin(bad_indices)\n",
    "        \n",
    "        # Apply the mask to the POINTS and LINES\n",
    "        filtered_points_gdf = gdf_points[good_mask].copy()\n",
    "        filtered_transects_gdf = gdf_transects[good_mask].copy()\n",
    "        \n",
    "        print(f\"  FILTERING COMPLETED: {len(filtered_points_gdf)} of {len(gdf_points)} points/transects retained.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during spatial filtering: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- Step 6: Save ---\n",
    "    try:\n",
    "        # 1. Save the FILTERED POINTS\n",
    "        print(f\"Saving filtered POINTS to: {POINTS_PATH} (Layer: {OUTPUT_POINTS_LAYER_NAME})\")\n",
    "        filtered_points_gdf.to_file(POINTS_PATH, layer=OUTPUT_POINTS_LAYER_NAME, driver=\"GPKG\")\n",
    "        \n",
    "        # 2. Save the FILTERED TRANSECTS\n",
    "        print(f\"Saving filtered TRANSECTS to: {POINTS_PATH} (Layer: {OUTPUT_TRANSECTS_LAYER_NAME})\")\n",
    "        filtered_transects_gdf.to_file(POINTS_PATH, layer=OUTPUT_TRANSECTS_LAYER_NAME, driver=\"GPKG\")\n",
    "        \n",
    "        # 3. Save the buffer\n",
    "        print(f\"Saving road buffer (Exclusion Zone) to: {OUTPUT_BUFFER_PATH} layer: roadsBuffer{BUFFER_DISTANCE_METERS}\")\n",
    "        exclusion_gdf.to_file(OUTPUT_BUFFER_PATH, layer= f\"roadsBuffer{BUFFER_DISTANCE_METERS}\", driver=\"GPKG\")\n",
    "        \n",
    "        \n",
    "        print(\"  Successfully saved.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR while saving the result files: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Helper function for the \"if empty\" case ---\n",
    "def generate_and_save_transects(points_gdf, output_path, layer_name):\n",
    "    \"\"\"Generates lines and saves them.\"\"\"\n",
    "    transect_geometries = []\n",
    "    for point in points_gdf.geometry:\n",
    "        start_x, start_y = point.x, point.y\n",
    "        end_x = start_x - TRANSECT_LENGTH_METERS\n",
    "        end_y = start_y\n",
    "        transect_geometries.append(LineString([(start_x, start_y), (end_x, end_y)]))\n",
    "    \n",
    "    gdf_transects = gpd.GeoDataFrame(geometry=transect_geometries, crs=points_gdf.crs)\n",
    "    gdf_transects.index = points_gdf.index\n",
    "    gdf_transects.to_file(output_path, layer=layer_name, driver=\"GPKG\")\n",
    "    print(f\"  Transects saved in: {output_path} (Layer: {layer_name})\")\n",
    "\n",
    "\n",
    "# --- 3. Execute Script ---\n",
    "if __name__ == \"__main__\":\n",
    "    success = filter_transects_near_roads()\n",
    "    \n",
    "    print(\"\\n---------------------------------\")\n",
    "    if success:\n",
    "        print(\"Script finished successfully.\")\n",
    "    else:\n",
    "        print(\"Script finished with errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81504f22",
   "metadata": {},
   "source": [
    "# Create Sample Points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "243b39cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Create QField Survey Points (MultiPoint Layout) ---\n",
      "Reading final start points: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg (Layer: noRoads_points_100m)\n",
      "  661 transect start points loaded.\n",
      "Generating MultiPoint layout for 661 transects...\n",
      "Creating final GeoDataFrame with 661 MultiPoint features...\n",
      "Saving survey points to: ..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg (Layer: final_survey_multipoints)\n",
      "  Successfully saved.\n",
      "\n",
      "---------------------------------\n",
      "Script finished successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "# Path to the GeoPackage\n",
    "POINTS_PATH = r\"..\\data\\edit\\Alba_dist_combined_v3\\fichtelforst.gpkg\"\n",
    "INPUT_LAYER_NAME = \"noRoads_points_100m\" \n",
    "\n",
    "# --- Definition of the Survey Design ---\n",
    "\n",
    "# Points along the main line (in meters from the western edge)\n",
    "# -10 = 10m into the disturbance area (East)\n",
    "# 0 = Western edge\n",
    "# 100 = 100m into the forest (West)\n",
    "TRANSECT_INTERVALS_M = [\n",
    "    -10, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 75, 100\n",
    "]\n",
    "# 14 intervals\n",
    "\n",
    "# Cross-offset (left/right or North/South) in meters\n",
    "CROSS_OFFSET_M = 1.5\n",
    "\n",
    "# --- Output ---\n",
    "# Name of the new layer for the MultiPoint features\n",
    "OUTPUT_LAYER_NAME = \"final_survey_multipoints\"\n",
    "\n",
    "# --- 2. Main Script ---\n",
    "\n",
    "def create_survey_multipoints():\n",
    "    print(f\"--- Create QField Survey Points (MultiPoint Layout) ---\")\n",
    "\n",
    "    # --- Step 1: Load Points ---\n",
    "    try:\n",
    "        print(f\"Reading final start points: {POINTS_PATH} (Layer: {INPUT_LAYER_NAME})\")\n",
    "        gdf_points = gpd.read_file(POINTS_PATH, layer=INPUT_LAYER_NAME)\n",
    "        print(f\"  {len(gdf_points)} transect start points loaded.\")\n",
    "        if gdf_points.empty:\n",
    "            print(\"ERROR: No points found in the layer. Aborting.\")\n",
    "            return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR while reading the point file/layer: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- Step 2: Create MultiPoint Geometries ---\n",
    "    print(f\"Generating MultiPoint layout for {len(gdf_points)} transects...\")\n",
    "    \n",
    "    all_transect_multipoints = [] \n",
    "\n",
    "    # .itertuples() reads all attributes\n",
    "    for row in gdf_points.itertuples(index=False): \n",
    "        \n",
    "        start_point_geom = row.geometry\n",
    "        start_x, start_y = start_point_geom.x, start_point_geom.y\n",
    "        \n",
    "        points_for_this_transect = [] \n",
    "        \n",
    "        # Inner loop: Iterate through all measurement distances\n",
    "        for distance_m in TRANSECT_INTERVALS_M:\n",
    "            \n",
    "            # X-coordinate: West is negative X.\n",
    "            # -10m becomes start_x + 10m\n",
    "            # +100m becomes start_x - 100m\n",
    "            center_x = start_x - distance_m \n",
    "            \n",
    "            # Point 1: On the line\n",
    "            p_center = Point(center_x, start_y)\n",
    "            \n",
    "            # Point 2: 1.5m \"left\" (North)\n",
    "            p_north = Point(center_x, start_y + CROSS_OFFSET_M)\n",
    "            \n",
    "            # Point 3: 1.5m \"right\" (South)\n",
    "            p_south = Point(center_x, start_y - CROSS_OFFSET_M)\n",
    "            \n",
    "            points_for_this_transect.extend([p_center, p_north, p_south])\n",
    "            \n",
    "        # (Loop finished: 14 intervals * 3 points = 42 points are collected)\n",
    "        \n",
    "        # Create the MultiPoint geometry\n",
    "        multipoint_geom = MultiPoint(points_for_this_transect)\n",
    "        \n",
    "    \n",
    "        \n",
    "        # 1. Convert the row ('namedtuple') into a dictionary.\n",
    "        #    automatically adopts all columns (PatchID, Area_ha, etc.)\n",
    "        new_data = row._asdict()\n",
    "        \n",
    "        # 2. Replace the OLD geometry (the start point) \n",
    "        #    with the MultiPoint geometry\n",
    "        new_data['geometry'] = multipoint_geom\n",
    "        \n",
    "        # 3. Append the complete dictionary to the list\n",
    "        all_transect_multipoints.append(new_data)\n",
    "\n",
    "\n",
    "    # --- Step 3: Create and Save New GeoDataFrame ---\n",
    "    try:\n",
    "        print(f\"Creating final GeoDataFrame with {len(all_transect_multipoints)} MultiPoint features...\")\n",
    "        \n",
    "        # Create the GeoDataFrame using the CRS of the points\n",
    "        gdf_final = gpd.GeoDataFrame(all_transect_multipoints, crs=gdf_points.crs)\n",
    "\n",
    "        print(f\"Saving survey points to: {POINTS_PATH} (Layer: {OUTPUT_LAYER_NAME})\")\n",
    "        gdf_final.to_file(POINTS_PATH, layer=OUTPUT_LAYER_NAME, driver=\"GPKG\")\n",
    "        \n",
    "        print(\"  Successfully saved.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR while creating or saving the GeoDataFrame: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- 3. Execute Script ---\n",
    "if __name__ == \"__main__\":\n",
    "    success = create_survey_multipoints()\n",
    "    \n",
    "    print(\"\\n---------------------------------\")\n",
    "    if success:\n",
    "        print(\"Script finished successfully.\")\n",
    "    else:\n",
    "        print(\"Script finished with errors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fichtelgebirge (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
